{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE: tuple = (128, 128)\n",
    "ROOT_TEST: str = './data/test'\n",
    "ROOT_TRAIN: str = './data/train'\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_folder = datasets.ImageFolder(root=ROOT_TRAIN, transform=transforms)\n",
    "test_folder = datasets.ImageFolder(root=ROOT_TEST, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_folder, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_folder, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_img(img: np.ndarray):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(8)\n",
    "    if img.shape[0] <= 3:\n",
    "        ax.imshow(np.swapaxes(img, 0, 2))\n",
    "    else:\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderAnomaly(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoencoderAnomaly, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 4, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2))\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4, 16, 2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, 2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = self.encoder(X)\n",
    "        return self.decoder(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, model, criterion, optimizer):\n",
    "    for epoch in range(epochs):\n",
    "        loss_batch = 0\n",
    "        for sample, _ in train_loader:\n",
    "            pred = model(sample)\n",
    "            loss = criterion(pred, sample)\n",
    "            loss_batch += loss.detach()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'{epoch}) Loss: {loss_batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoencoderAnomaly()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Loss: 2.7768945693969727\n",
      "1) Loss: 2.773282051086426\n",
      "2) Loss: 2.769437313079834\n",
      "3) Loss: 2.7651326656341553\n",
      "4) Loss: 2.760310173034668\n",
      "5) Loss: 2.7546396255493164\n",
      "6) Loss: 2.7472212314605713\n",
      "7) Loss: 2.737391471862793\n",
      "8) Loss: 2.724660634994507\n",
      "9) Loss: 2.7098681926727295\n",
      "10) Loss: 2.6955394744873047\n",
      "11) Loss: 2.6842665672302246\n",
      "12) Loss: 2.6771397590637207\n",
      "13) Loss: 2.671403408050537\n",
      "14) Loss: 2.6643993854522705\n",
      "15) Loss: 2.658352851867676\n",
      "16) Loss: 2.6534981727600098\n",
      "17) Loss: 2.6497273445129395\n",
      "18) Loss: 2.6459977626800537\n",
      "19) Loss: 2.643131732940674\n",
      "20) Loss: 2.641004800796509\n",
      "21) Loss: 2.639070510864258\n",
      "22) Loss: 2.6373894214630127\n",
      "23) Loss: 2.636362075805664\n",
      "24) Loss: 2.6353845596313477\n",
      "25) Loss: 2.634629487991333\n",
      "26) Loss: 2.633758306503296\n",
      "27) Loss: 2.6333062648773193\n",
      "28) Loss: 2.6326823234558105\n",
      "29) Loss: 2.632275342941284\n",
      "30) Loss: 2.632141351699829\n",
      "31) Loss: 2.6317856311798096\n",
      "32) Loss: 2.631682872772217\n",
      "33) Loss: 2.6314425468444824\n",
      "34) Loss: 2.6312856674194336\n",
      "35) Loss: 2.631187677383423\n",
      "36) Loss: 2.631175994873047\n",
      "37) Loss: 2.630990505218506\n",
      "38) Loss: 2.6307759284973145\n",
      "39) Loss: 2.6311862468719482\n",
      "40) Loss: 2.630913496017456\n",
      "41) Loss: 2.6308529376983643\n",
      "42) Loss: 2.630833148956299\n",
      "43) Loss: 2.6309702396392822\n",
      "44) Loss: 2.630929946899414\n",
      "45) Loss: 2.6306509971618652\n",
      "46) Loss: 2.6306772232055664\n",
      "47) Loss: 2.630760669708252\n",
      "48) Loss: 2.6307690143585205\n",
      "49) Loss: 2.6308181285858154\n"
     ]
    }
   ],
   "source": [
    "train_model(50, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Loss: 2.630657434463501\n",
      "1) Loss: 2.630586862564087\n",
      "2) Loss: 2.630702495574951\n",
      "3) Loss: 2.6305885314941406\n",
      "4) Loss: 2.6306395530700684\n",
      "5) Loss: 2.630746841430664\n",
      "6) Loss: 2.6306209564208984\n",
      "7) Loss: 2.630605697631836\n",
      "8) Loss: 2.630537986755371\n",
      "9) Loss: 2.6306400299072266\n",
      "10) Loss: 2.630643844604492\n",
      "11) Loss: 2.6305086612701416\n",
      "12) Loss: 2.6304516792297363\n",
      "13) Loss: 2.6304821968078613\n",
      "14) Loss: 2.6305289268493652\n",
      "15) Loss: 2.630631446838379\n",
      "16) Loss: 2.6304914951324463\n",
      "17) Loss: 2.6307032108306885\n",
      "18) Loss: 2.6304140090942383\n",
      "19) Loss: 2.630546808242798\n",
      "20) Loss: 2.6304256916046143\n",
      "21) Loss: 2.630448818206787\n",
      "22) Loss: 2.6304619312286377\n",
      "23) Loss: 2.6304049491882324\n",
      "24) Loss: 2.630359649658203\n",
      "25) Loss: 2.630424976348877\n",
      "26) Loss: 2.630340576171875\n",
      "27) Loss: 2.63027286529541\n",
      "28) Loss: 2.6303701400756836\n",
      "29) Loss: 2.6302361488342285\n",
      "30) Loss: 2.6301581859588623\n",
      "31) Loss: 2.6302249431610107\n",
      "32) Loss: 2.630133867263794\n",
      "33) Loss: 2.630181312561035\n",
      "34) Loss: 2.6300034523010254\n",
      "35) Loss: 2.6300296783447266\n",
      "36) Loss: 2.6303014755249023\n",
      "37) Loss: 2.6300017833709717\n",
      "38) Loss: 2.6301074028015137\n",
      "39) Loss: 2.6300745010375977\n",
      "40) Loss: 2.6300764083862305\n",
      "41) Loss: 2.630190372467041\n",
      "42) Loss: 2.6299822330474854\n",
      "43) Loss: 2.630030632019043\n",
      "44) Loss: 2.629998207092285\n",
      "45) Loss: 2.6298952102661133\n",
      "46) Loss: 2.630000591278076\n",
      "47) Loss: 2.6300556659698486\n",
      "48) Loss: 2.62992525100708\n",
      "49) Loss: 2.629849910736084\n",
      "50) Loss: 2.629987955093384\n",
      "51) Loss: 2.629903793334961\n",
      "52) Loss: 2.62992787361145\n",
      "53) Loss: 2.629629135131836\n",
      "54) Loss: 2.6297659873962402\n",
      "55) Loss: 2.629716396331787\n",
      "56) Loss: 2.6297359466552734\n",
      "57) Loss: 2.629746437072754\n",
      "58) Loss: 2.629775047302246\n",
      "59) Loss: 2.6296916007995605\n",
      "60) Loss: 2.6296377182006836\n",
      "61) Loss: 2.629884719848633\n",
      "62) Loss: 2.629535675048828\n",
      "63) Loss: 2.6294846534729004\n",
      "64) Loss: 2.629526138305664\n",
      "65) Loss: 2.6296474933624268\n",
      "66) Loss: 2.62939715385437\n",
      "67) Loss: 2.629523992538452\n",
      "68) Loss: 2.62939190864563\n",
      "69) Loss: 2.629255771636963\n",
      "70) Loss: 2.629315137863159\n",
      "71) Loss: 2.629284381866455\n",
      "72) Loss: 2.6293609142303467\n",
      "73) Loss: 2.6292779445648193\n",
      "74) Loss: 2.6292295455932617\n",
      "75) Loss: 2.62918758392334\n",
      "76) Loss: 2.6290791034698486\n",
      "77) Loss: 2.6290369033813477\n",
      "78) Loss: 2.628988265991211\n",
      "79) Loss: 2.628880739212036\n",
      "80) Loss: 2.6288464069366455\n",
      "81) Loss: 2.6287574768066406\n",
      "82) Loss: 2.6288418769836426\n",
      "83) Loss: 2.628617763519287\n",
      "84) Loss: 2.628709316253662\n",
      "85) Loss: 2.628605842590332\n",
      "86) Loss: 2.6283771991729736\n",
      "87) Loss: 2.628347396850586\n",
      "88) Loss: 2.6283388137817383\n",
      "89) Loss: 2.6281442642211914\n",
      "90) Loss: 2.628263473510742\n",
      "91) Loss: 2.6280312538146973\n",
      "92) Loss: 2.628098487854004\n",
      "93) Loss: 2.6280415058135986\n",
      "94) Loss: 2.6279244422912598\n",
      "95) Loss: 2.6279335021972656\n",
      "96) Loss: 2.6275529861450195\n",
      "97) Loss: 2.62756085395813\n",
      "98) Loss: 2.627265691757202\n",
      "99) Loss: 2.627455949783325\n"
     ]
    }
   ],
   "source": [
    "train_model(100, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Loss: 2.627014636993408\n",
      "1) Loss: 2.6270885467529297\n",
      "2) Loss: 2.6268937587738037\n",
      "3) Loss: 2.6268773078918457\n",
      "4) Loss: 2.626683235168457\n",
      "5) Loss: 2.6264703273773193\n",
      "6) Loss: 2.6263158321380615\n",
      "7) Loss: 2.6261937618255615\n",
      "8) Loss: 2.6258773803710938\n",
      "9) Loss: 2.6259920597076416\n",
      "10) Loss: 2.6256332397460938\n",
      "11) Loss: 2.625359535217285\n",
      "12) Loss: 2.6250696182250977\n",
      "13) Loss: 2.6246516704559326\n",
      "14) Loss: 2.6246161460876465\n",
      "15) Loss: 2.6243131160736084\n",
      "16) Loss: 2.6240758895874023\n",
      "17) Loss: 2.623504638671875\n",
      "18) Loss: 2.62307071685791\n",
      "19) Loss: 2.622715950012207\n",
      "20) Loss: 2.622292995452881\n",
      "21) Loss: 2.621829032897949\n",
      "22) Loss: 2.621274471282959\n",
      "23) Loss: 2.620908498764038\n",
      "24) Loss: 2.620305061340332\n",
      "25) Loss: 2.619925022125244\n",
      "26) Loss: 2.6191539764404297\n",
      "27) Loss: 2.6187610626220703\n",
      "28) Loss: 2.6183371543884277\n",
      "29) Loss: 2.6178038120269775\n",
      "30) Loss: 2.617363452911377\n",
      "31) Loss: 2.6169939041137695\n",
      "32) Loss: 2.6166205406188965\n",
      "33) Loss: 2.6163277626037598\n",
      "34) Loss: 2.615891933441162\n",
      "35) Loss: 2.615557909011841\n",
      "36) Loss: 2.615192174911499\n",
      "37) Loss: 2.614967107772827\n",
      "38) Loss: 2.6146819591522217\n",
      "39) Loss: 2.6143887042999268\n",
      "40) Loss: 2.614161491394043\n",
      "41) Loss: 2.6141045093536377\n",
      "42) Loss: 2.613931179046631\n",
      "43) Loss: 2.614132881164551\n",
      "44) Loss: 2.613961935043335\n",
      "45) Loss: 2.6136178970336914\n",
      "46) Loss: 2.613365411758423\n",
      "47) Loss: 2.613311767578125\n",
      "48) Loss: 2.6130928993225098\n",
      "49) Loss: 2.613101005554199\n",
      "50) Loss: 2.6130528450012207\n",
      "51) Loss: 2.612757682800293\n",
      "52) Loss: 2.6129045486450195\n",
      "53) Loss: 2.6128783226013184\n",
      "54) Loss: 2.612623453140259\n",
      "55) Loss: 2.612832546234131\n",
      "56) Loss: 2.6125600337982178\n",
      "57) Loss: 2.6125364303588867\n",
      "58) Loss: 2.6125447750091553\n",
      "59) Loss: 2.6123976707458496\n",
      "60) Loss: 2.6123292446136475\n",
      "61) Loss: 2.612434148788452\n",
      "62) Loss: 2.6122264862060547\n",
      "63) Loss: 2.612180709838867\n",
      "64) Loss: 2.6121182441711426\n",
      "65) Loss: 2.6120057106018066\n",
      "66) Loss: 2.612183094024658\n",
      "67) Loss: 2.6120808124542236\n",
      "68) Loss: 2.612032413482666\n",
      "69) Loss: 2.612149238586426\n",
      "70) Loss: 2.612016201019287\n",
      "71) Loss: 2.6119916439056396\n",
      "72) Loss: 2.61199951171875\n",
      "73) Loss: 2.6118431091308594\n",
      "74) Loss: 2.6119391918182373\n",
      "75) Loss: 2.611987590789795\n",
      "76) Loss: 2.6118593215942383\n",
      "77) Loss: 2.6117262840270996\n",
      "78) Loss: 2.611814498901367\n",
      "79) Loss: 2.6118757724761963\n",
      "80) Loss: 2.611876964569092\n",
      "81) Loss: 2.6119000911712646\n",
      "82) Loss: 2.612006664276123\n",
      "83) Loss: 2.611870050430298\n",
      "84) Loss: 2.611903429031372\n",
      "85) Loss: 2.6118316650390625\n",
      "86) Loss: 2.6118524074554443\n",
      "87) Loss: 2.6115939617156982\n",
      "88) Loss: 2.6116747856140137\n",
      "89) Loss: 2.6116600036621094\n",
      "90) Loss: 2.611752986907959\n",
      "91) Loss: 2.61163592338562\n",
      "92) Loss: 2.6114959716796875\n",
      "93) Loss: 2.611607074737549\n",
      "94) Loss: 2.6115317344665527\n",
      "95) Loss: 2.611682415008545\n",
      "96) Loss: 2.6119155883789062\n",
      "97) Loss: 2.6116814613342285\n",
      "98) Loss: 2.6114370822906494\n",
      "99) Loss: 2.6116514205932617\n",
      "100) Loss: 2.6115925312042236\n",
      "101) Loss: 2.6116783618927\n",
      "102) Loss: 2.6114327907562256\n",
      "103) Loss: 2.611635446548462\n",
      "104) Loss: 2.6116321086883545\n",
      "105) Loss: 2.611499309539795\n",
      "106) Loss: 2.61149001121521\n",
      "107) Loss: 2.611419677734375\n",
      "108) Loss: 2.611684799194336\n",
      "109) Loss: 2.6116695404052734\n",
      "110) Loss: 2.611581802368164\n",
      "111) Loss: 2.611532688140869\n",
      "112) Loss: 2.6116271018981934\n",
      "113) Loss: 2.6115944385528564\n",
      "114) Loss: 2.611609935760498\n",
      "115) Loss: 2.6115481853485107\n",
      "116) Loss: 2.6114931106567383\n",
      "117) Loss: 2.6113603115081787\n",
      "118) Loss: 2.6113998889923096\n",
      "119) Loss: 2.611393928527832\n",
      "120) Loss: 2.611527919769287\n",
      "121) Loss: 2.611544609069824\n",
      "122) Loss: 2.611243724822998\n",
      "123) Loss: 2.6115658283233643\n",
      "124) Loss: 2.611501455307007\n",
      "125) Loss: 2.6113855838775635\n",
      "126) Loss: 2.6114039421081543\n",
      "127) Loss: 2.611488103866577\n",
      "128) Loss: 2.6112773418426514\n",
      "129) Loss: 2.611355781555176\n",
      "130) Loss: 2.6115121841430664\n",
      "131) Loss: 2.6112799644470215\n",
      "132) Loss: 2.6115036010742188\n",
      "133) Loss: 2.611602544784546\n",
      "134) Loss: 2.6114230155944824\n",
      "135) Loss: 2.611440658569336\n",
      "136) Loss: 2.6114635467529297\n",
      "137) Loss: 2.6114985942840576\n",
      "138) Loss: 2.6115665435791016\n",
      "139) Loss: 2.611542224884033\n",
      "140) Loss: 2.611431121826172\n",
      "141) Loss: 2.611328363418579\n",
      "142) Loss: 2.6114845275878906\n",
      "143) Loss: 2.61132550239563\n",
      "144) Loss: 2.611255168914795\n",
      "145) Loss: 2.611506462097168\n",
      "146) Loss: 2.611562967300415\n",
      "147) Loss: 2.611471652984619\n",
      "148) Loss: 2.6114070415496826\n",
      "149) Loss: 2.6112687587738037\n",
      "150) Loss: 2.611415147781372\n",
      "151) Loss: 2.6112782955169678\n",
      "152) Loss: 2.6114044189453125\n",
      "153) Loss: 2.611400604248047\n",
      "154) Loss: 2.6114633083343506\n",
      "155) Loss: 2.611476182937622\n",
      "156) Loss: 2.611257314682007\n",
      "157) Loss: 2.611469268798828\n",
      "158) Loss: 2.611320734024048\n",
      "159) Loss: 2.6113924980163574\n",
      "160) Loss: 2.611396312713623\n",
      "161) Loss: 2.611337900161743\n",
      "162) Loss: 2.611309051513672\n",
      "163) Loss: 2.611284017562866\n",
      "164) Loss: 2.611354112625122\n",
      "165) Loss: 2.61129093170166\n",
      "166) Loss: 2.61142635345459\n",
      "167) Loss: 2.6111605167388916\n",
      "168) Loss: 2.6114182472229004\n",
      "169) Loss: 2.6113967895507812\n",
      "170) Loss: 2.6112966537475586\n",
      "171) Loss: 2.6113693714141846\n",
      "172) Loss: 2.611328363418579\n",
      "173) Loss: 2.6113061904907227\n",
      "174) Loss: 2.611321449279785\n",
      "175) Loss: 2.611392021179199\n",
      "176) Loss: 2.611161470413208\n",
      "177) Loss: 2.6114437580108643\n",
      "178) Loss: 2.6113476753234863\n",
      "179) Loss: 2.6113245487213135\n",
      "180) Loss: 2.611398696899414\n",
      "181) Loss: 2.611254930496216\n",
      "182) Loss: 2.6114141941070557\n",
      "183) Loss: 2.611426591873169\n",
      "184) Loss: 2.6114542484283447\n",
      "185) Loss: 2.611166000366211\n",
      "186) Loss: 2.6112306118011475\n",
      "187) Loss: 2.6112802028656006\n",
      "188) Loss: 2.6114392280578613\n",
      "189) Loss: 2.6112632751464844\n",
      "190) Loss: 2.6112849712371826\n",
      "191) Loss: 2.611264705657959\n",
      "192) Loss: 2.6112887859344482\n",
      "193) Loss: 2.611340284347534\n",
      "194) Loss: 2.611304759979248\n",
      "195) Loss: 2.6114373207092285\n",
      "196) Loss: 2.611267566680908\n",
      "197) Loss: 2.6113011837005615\n",
      "198) Loss: 2.6115410327911377\n",
      "199) Loss: 2.611459493637085\n",
      "200) Loss: 2.6111936569213867\n",
      "201) Loss: 2.611147165298462\n",
      "202) Loss: 2.6112804412841797\n",
      "203) Loss: 2.611285924911499\n",
      "204) Loss: 2.611253499984741\n",
      "205) Loss: 2.611154556274414\n",
      "206) Loss: 2.6111817359924316\n",
      "207) Loss: 2.6112260818481445\n",
      "208) Loss: 2.611330986022949\n",
      "209) Loss: 2.6111900806427\n",
      "210) Loss: 2.6112561225891113\n",
      "211) Loss: 2.611254930496216\n",
      "212) Loss: 2.6113433837890625\n",
      "213) Loss: 2.611238479614258\n",
      "214) Loss: 2.611293315887451\n",
      "215) Loss: 2.6112115383148193\n",
      "216) Loss: 2.6112124919891357\n",
      "217) Loss: 2.6112265586853027\n",
      "218) Loss: 2.6114115715026855\n",
      "219) Loss: 2.6112074851989746\n",
      "220) Loss: 2.611180067062378\n",
      "221) Loss: 2.611201286315918\n",
      "222) Loss: 2.6113462448120117\n",
      "223) Loss: 2.611311674118042\n",
      "224) Loss: 2.6111714839935303\n",
      "225) Loss: 2.6110472679138184\n",
      "226) Loss: 2.611133098602295\n",
      "227) Loss: 2.6112077236175537\n",
      "228) Loss: 2.611238718032837\n",
      "229) Loss: 2.611283302307129\n",
      "230) Loss: 2.6111788749694824\n",
      "231) Loss: 2.6112937927246094\n",
      "232) Loss: 2.6110005378723145\n",
      "233) Loss: 2.6114180088043213\n",
      "234) Loss: 2.6113405227661133\n",
      "235) Loss: 2.6114492416381836\n",
      "236) Loss: 2.6112449169158936\n",
      "237) Loss: 2.611198902130127\n",
      "238) Loss: 2.6111536026000977\n",
      "239) Loss: 2.611330986022949\n",
      "240) Loss: 2.611208915710449\n",
      "241) Loss: 2.611164093017578\n",
      "242) Loss: 2.611112594604492\n",
      "243) Loss: 2.6112453937530518\n",
      "244) Loss: 2.611201524734497\n",
      "245) Loss: 2.6112852096557617\n",
      "246) Loss: 2.6113624572753906\n",
      "247) Loss: 2.6110646724700928\n",
      "248) Loss: 2.6111693382263184\n",
      "249) Loss: 2.6111836433410645\n",
      "250) Loss: 2.611250877380371\n",
      "251) Loss: 2.610966444015503\n",
      "252) Loss: 2.611304998397827\n",
      "253) Loss: 2.61136794090271\n",
      "254) Loss: 2.6112046241760254\n",
      "255) Loss: 2.611313819885254\n",
      "256) Loss: 2.6111764907836914\n",
      "257) Loss: 2.6111392974853516\n",
      "258) Loss: 2.610926628112793\n",
      "259) Loss: 2.61130690574646\n",
      "260) Loss: 2.61112117767334\n",
      "261) Loss: 2.611161470413208\n",
      "262) Loss: 2.6111888885498047\n",
      "263) Loss: 2.611254930496216\n",
      "264) Loss: 2.611156702041626\n",
      "265) Loss: 2.6111817359924316\n",
      "266) Loss: 2.6110944747924805\n",
      "267) Loss: 2.611173152923584\n",
      "268) Loss: 2.61110258102417\n",
      "269) Loss: 2.6112027168273926\n",
      "270) Loss: 2.611112117767334\n",
      "271) Loss: 2.611262321472168\n",
      "272) Loss: 2.6111538410186768\n",
      "273) Loss: 2.6110262870788574\n",
      "274) Loss: 2.611262798309326\n",
      "275) Loss: 2.611090660095215\n",
      "276) Loss: 2.611124038696289\n",
      "277) Loss: 2.6111912727355957\n",
      "278) Loss: 2.6111974716186523\n",
      "279) Loss: 2.61104154586792\n",
      "280) Loss: 2.611118793487549\n",
      "281) Loss: 2.6111512184143066\n",
      "282) Loss: 2.611311435699463\n",
      "283) Loss: 2.611262321472168\n",
      "284) Loss: 2.6112234592437744\n",
      "285) Loss: 2.6110000610351562\n",
      "286) Loss: 2.6112585067749023\n",
      "287) Loss: 2.611293315887451\n",
      "288) Loss: 2.611178159713745\n",
      "289) Loss: 2.6111247539520264\n",
      "290) Loss: 2.61114764213562\n",
      "291) Loss: 2.6111459732055664\n",
      "292) Loss: 2.611184597015381\n",
      "293) Loss: 2.6112515926361084\n",
      "294) Loss: 2.611130952835083\n",
      "295) Loss: 2.611036777496338\n",
      "296) Loss: 2.611067533493042\n",
      "297) Loss: 2.6111602783203125\n",
      "298) Loss: 2.611180305480957\n",
      "299) Loss: 2.6111741065979004\n",
      "300) Loss: 2.6111083030700684\n",
      "301) Loss: 2.611238956451416\n",
      "302) Loss: 2.6111979484558105\n",
      "303) Loss: 2.6110928058624268\n",
      "304) Loss: 2.6111044883728027\n",
      "305) Loss: 2.611114263534546\n",
      "306) Loss: 2.6110634803771973\n",
      "307) Loss: 2.611124277114868\n",
      "308) Loss: 2.611107110977173\n",
      "309) Loss: 2.6110153198242188\n",
      "310) Loss: 2.6112070083618164\n",
      "311) Loss: 2.6110472679138184\n",
      "312) Loss: 2.6111786365509033\n",
      "313) Loss: 2.6109962463378906\n",
      "314) Loss: 2.611119508743286\n",
      "315) Loss: 2.611083507537842\n",
      "316) Loss: 2.6110219955444336\n",
      "317) Loss: 2.6109261512756348\n",
      "318) Loss: 2.6110708713531494\n",
      "319) Loss: 2.611065626144409\n",
      "320) Loss: 2.611084461212158\n",
      "321) Loss: 2.6109797954559326\n",
      "322) Loss: 2.611213207244873\n",
      "323) Loss: 2.6111903190612793\n",
      "324) Loss: 2.611212730407715\n",
      "325) Loss: 2.6111598014831543\n",
      "326) Loss: 2.611309051513672\n",
      "327) Loss: 2.6111509799957275\n",
      "328) Loss: 2.611250400543213\n",
      "329) Loss: 2.611104965209961\n",
      "330) Loss: 2.6111738681793213\n",
      "331) Loss: 2.610931396484375\n",
      "332) Loss: 2.6110129356384277\n",
      "333) Loss: 2.6111621856689453\n",
      "334) Loss: 2.6110029220581055\n",
      "335) Loss: 2.611253261566162\n",
      "336) Loss: 2.611104965209961\n",
      "337) Loss: 2.6110501289367676\n",
      "338) Loss: 2.611039161682129\n",
      "339) Loss: 2.61103892326355\n",
      "340) Loss: 2.610934019088745\n",
      "341) Loss: 2.6111104488372803\n",
      "342) Loss: 2.6111130714416504\n",
      "343) Loss: 2.6110291481018066\n",
      "344) Loss: 2.6109955310821533\n",
      "345) Loss: 2.611020803451538\n",
      "346) Loss: 2.611177921295166\n",
      "347) Loss: 2.6111044883728027\n",
      "348) Loss: 2.610973834991455\n",
      "349) Loss: 2.6111788749694824\n",
      "350) Loss: 2.61104154586792\n",
      "351) Loss: 2.6110284328460693\n",
      "352) Loss: 2.611081123352051\n",
      "353) Loss: 2.6109418869018555\n",
      "354) Loss: 2.610988140106201\n",
      "355) Loss: 2.611173152923584\n",
      "356) Loss: 2.6112377643585205\n",
      "357) Loss: 2.6110448837280273\n",
      "358) Loss: 2.61095929145813\n",
      "359) Loss: 2.610999584197998\n",
      "360) Loss: 2.6110754013061523\n",
      "361) Loss: 2.6109678745269775\n",
      "362) Loss: 2.610945701599121\n",
      "363) Loss: 2.6109156608581543\n",
      "364) Loss: 2.6110644340515137\n",
      "365) Loss: 2.611111640930176\n",
      "366) Loss: 2.6110422611236572\n",
      "367) Loss: 2.6110053062438965\n",
      "368) Loss: 2.6110000610351562\n",
      "369) Loss: 2.6109156608581543\n",
      "370) Loss: 2.61116361618042\n",
      "371) Loss: 2.6110219955444336\n",
      "372) Loss: 2.611062526702881\n",
      "373) Loss: 2.6109752655029297\n",
      "374) Loss: 2.6111111640930176\n",
      "375) Loss: 2.6110475063323975\n",
      "376) Loss: 2.6110477447509766\n",
      "377) Loss: 2.6110472679138184\n",
      "378) Loss: 2.611053943634033\n",
      "379) Loss: 2.611138105392456\n",
      "380) Loss: 2.6110846996307373\n",
      "381) Loss: 2.611117124557495\n",
      "382) Loss: 2.6111087799072266\n",
      "383) Loss: 2.6109936237335205\n",
      "384) Loss: 2.6110310554504395\n",
      "385) Loss: 2.6109490394592285\n",
      "386) Loss: 2.610973834991455\n",
      "387) Loss: 2.6109695434570312\n",
      "388) Loss: 2.6110024452209473\n",
      "389) Loss: 2.610790252685547\n",
      "390) Loss: 2.611011028289795\n",
      "391) Loss: 2.6109116077423096\n",
      "392) Loss: 2.6110482215881348\n",
      "393) Loss: 2.610999822616577\n",
      "394) Loss: 2.611090660095215\n",
      "395) Loss: 2.6109964847564697\n",
      "396) Loss: 2.6110763549804688\n",
      "397) Loss: 2.6111297607421875\n",
      "398) Loss: 2.610900640487671\n",
      "399) Loss: 2.611112356185913\n",
      "400) Loss: 2.611016273498535\n",
      "401) Loss: 2.610983371734619\n",
      "402) Loss: 2.611001968383789\n",
      "403) Loss: 2.6109747886657715\n",
      "404) Loss: 2.6110925674438477\n",
      "405) Loss: 2.610877752304077\n",
      "406) Loss: 2.6110894680023193\n",
      "407) Loss: 2.611010789871216\n",
      "408) Loss: 2.6109776496887207\n",
      "409) Loss: 2.6109604835510254\n",
      "410) Loss: 2.611135959625244\n",
      "411) Loss: 2.611022710800171\n",
      "412) Loss: 2.6110875606536865\n",
      "413) Loss: 2.6110358238220215\n",
      "414) Loss: 2.6110267639160156\n",
      "415) Loss: 2.6108808517456055\n",
      "416) Loss: 2.610907554626465\n",
      "417) Loss: 2.610776662826538\n",
      "418) Loss: 2.61106276512146\n",
      "419) Loss: 2.6111087799072266\n",
      "420) Loss: 2.611022472381592\n",
      "421) Loss: 2.611097812652588\n",
      "422) Loss: 2.6111254692077637\n",
      "423) Loss: 2.6109580993652344\n",
      "424) Loss: 2.611110210418701\n",
      "425) Loss: 2.6110215187072754\n",
      "426) Loss: 2.611100912094116\n",
      "427) Loss: 2.611011028289795\n",
      "428) Loss: 2.610914468765259\n",
      "429) Loss: 2.610900640487671\n",
      "430) Loss: 2.6109800338745117\n",
      "431) Loss: 2.611008644104004\n",
      "432) Loss: 2.6109352111816406\n",
      "433) Loss: 2.6109673976898193\n",
      "434) Loss: 2.6108274459838867\n",
      "435) Loss: 2.6107852458953857\n",
      "436) Loss: 2.6111092567443848\n",
      "437) Loss: 2.6110646724700928\n",
      "438) Loss: 2.6110126972198486\n",
      "439) Loss: 2.6111156940460205\n",
      "440) Loss: 2.610999584197998\n",
      "441) Loss: 2.6110377311706543\n",
      "442) Loss: 2.610926628112793\n",
      "443) Loss: 2.6108758449554443\n",
      "444) Loss: 2.6109366416931152\n",
      "445) Loss: 2.6110377311706543\n",
      "446) Loss: 2.610905885696411\n",
      "447) Loss: 2.6110668182373047\n",
      "448) Loss: 2.61094331741333\n",
      "449) Loss: 2.610934257507324\n",
      "450) Loss: 2.6108317375183105\n",
      "451) Loss: 2.6110379695892334\n",
      "452) Loss: 2.6108646392822266\n",
      "453) Loss: 2.611021041870117\n",
      "454) Loss: 2.6110150814056396\n",
      "455) Loss: 2.6107468605041504\n",
      "456) Loss: 2.610841989517212\n",
      "457) Loss: 2.6109254360198975\n",
      "458) Loss: 2.6109426021575928\n",
      "459) Loss: 2.6107466220855713\n",
      "460) Loss: 2.610919713973999\n",
      "461) Loss: 2.610898494720459\n",
      "462) Loss: 2.6108591556549072\n",
      "463) Loss: 2.6110148429870605\n",
      "464) Loss: 2.6107869148254395\n",
      "465) Loss: 2.6108663082122803\n",
      "466) Loss: 2.61091685295105\n",
      "467) Loss: 2.6110148429870605\n",
      "468) Loss: 2.6109073162078857\n",
      "469) Loss: 2.611074924468994\n",
      "470) Loss: 2.6109869480133057\n",
      "471) Loss: 2.611016273498535\n",
      "472) Loss: 2.610853672027588\n",
      "473) Loss: 2.610896587371826\n",
      "474) Loss: 2.6110904216766357\n",
      "475) Loss: 2.6109371185302734\n",
      "476) Loss: 2.6109354496002197\n",
      "477) Loss: 2.6108415126800537\n",
      "478) Loss: 2.6110587120056152\n",
      "479) Loss: 2.610872268676758\n",
      "480) Loss: 2.610839366912842\n",
      "481) Loss: 2.6108996868133545\n",
      "482) Loss: 2.6108949184417725\n",
      "483) Loss: 2.6109399795532227\n",
      "484) Loss: 2.6109938621520996\n",
      "485) Loss: 2.610921621322632\n",
      "486) Loss: 2.6109700202941895\n",
      "487) Loss: 2.6110005378723145\n",
      "488) Loss: 2.6110522747039795\n",
      "489) Loss: 2.611023187637329\n",
      "490) Loss: 2.6109585762023926\n",
      "491) Loss: 2.61098575592041\n",
      "492) Loss: 2.6110033988952637\n",
      "493) Loss: 2.6109609603881836\n",
      "494) Loss: 2.6108264923095703\n",
      "495) Loss: 2.610734224319458\n",
      "496) Loss: 2.6109066009521484\n",
      "497) Loss: 2.6110424995422363\n",
      "498) Loss: 2.610879421234131\n",
      "499) Loss: 2.6108808517456055\n"
     ]
    }
   ],
   "source": [
    "train_model(500, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Loss: 0.0038176551461219788\n",
      "1) Loss: 0.0038187899626791477\n",
      "2) Loss: 0.0038211180362850428\n",
      "3) Loss: 0.0038201999850571156\n",
      "4) Loss: 0.003821194637566805\n",
      "5) Loss: 0.0038091291207820177\n",
      "6) Loss: 0.003824851242825389\n",
      "7) Loss: 0.0038094096817076206\n",
      "8) Loss: 0.0038119805976748466\n",
      "9) Loss: 0.00381387397646904\n",
      "10) Loss: 0.0038117270451039076\n",
      "11) Loss: 0.0038063875399529934\n",
      "12) Loss: 0.003819655627012253\n",
      "13) Loss: 0.003815714968368411\n",
      "14) Loss: 0.003813754767179489\n",
      "15) Loss: 0.003810954513028264\n",
      "16) Loss: 0.003803413826972246\n",
      "17) Loss: 0.0038173971697688103\n",
      "18) Loss: 0.003818607423454523\n",
      "19) Loss: 0.0038177981041371822\n",
      "20) Loss: 0.0038093975745141506\n",
      "21) Loss: 0.003812055103480816\n",
      "22) Loss: 0.0038093063049018383\n",
      "23) Loss: 0.00380346248857677\n",
      "24) Loss: 0.0038065859116613865\n",
      "25) Loss: 0.003817946882918477\n",
      "26) Loss: 0.0038002505898475647\n",
      "27) Loss: 0.0038152658380568027\n",
      "28) Loss: 0.003810502588748932\n",
      "29) Loss: 0.003810497000813484\n",
      "30) Loss: 0.0038027004338800907\n",
      "31) Loss: 0.003799157217144966\n",
      "32) Loss: 0.003819133620709181\n",
      "33) Loss: 0.0038030073046684265\n",
      "34) Loss: 0.0038000482600182295\n",
      "35) Loss: 0.0038110094610601664\n",
      "36) Loss: 0.003804985899478197\n",
      "37) Loss: 0.003809456480666995\n",
      "38) Loss: 0.003819063538685441\n",
      "39) Loss: 0.0038094245828688145\n",
      "40) Loss: 0.0037918437737971544\n",
      "41) Loss: 0.003810067428275943\n",
      "42) Loss: 0.0038119342643767595\n",
      "43) Loss: 0.0038021206855773926\n",
      "44) Loss: 0.0038106655701994896\n",
      "45) Loss: 0.003814355004578829\n",
      "46) Loss: 0.003816355485469103\n",
      "47) Loss: 0.003805361920967698\n",
      "48) Loss: 0.0037947578821331263\n",
      "49) Loss: 0.003801621962338686\n",
      "50) Loss: 0.003804052248597145\n",
      "51) Loss: 0.0037948302924633026\n",
      "52) Loss: 0.0038033113814890385\n",
      "53) Loss: 0.0038040042854845524\n",
      "54) Loss: 0.0038116544019430876\n",
      "55) Loss: 0.003802277846261859\n",
      "56) Loss: 0.0037943138740956783\n",
      "57) Loss: 0.003802753519266844\n",
      "58) Loss: 0.003809835761785507\n",
      "59) Loss: 0.003803458996117115\n",
      "60) Loss: 0.0037870737724006176\n",
      "61) Loss: 0.003799065249040723\n",
      "62) Loss: 0.0038004680536687374\n",
      "63) Loss: 0.003796397242695093\n",
      "64) Loss: 0.0038036599289625883\n",
      "65) Loss: 0.0038046855479478836\n",
      "66) Loss: 0.003798804711550474\n",
      "67) Loss: 0.003805765649303794\n",
      "68) Loss: 0.0037986664101481438\n",
      "69) Loss: 0.003792327828705311\n",
      "70) Loss: 0.0038078695069998503\n",
      "71) Loss: 0.0038046957924962044\n",
      "72) Loss: 0.0037956703454256058\n",
      "73) Loss: 0.0037840865552425385\n",
      "74) Loss: 0.003793020499870181\n",
      "75) Loss: 0.003786643035709858\n",
      "76) Loss: 0.0037791237700730562\n",
      "77) Loss: 0.00379003444686532\n",
      "78) Loss: 0.003797949990257621\n",
      "79) Loss: 0.0037933331914246082\n",
      "80) Loss: 0.003795352065935731\n",
      "81) Loss: 0.003790314542129636\n",
      "82) Loss: 0.0037828690838068724\n",
      "83) Loss: 0.003790919203311205\n",
      "84) Loss: 0.003795388387516141\n",
      "85) Loss: 0.0037927087396383286\n",
      "86) Loss: 0.0037802518345415592\n",
      "87) Loss: 0.00379212130792439\n",
      "88) Loss: 0.003794575110077858\n",
      "89) Loss: 0.0037917988374829292\n",
      "90) Loss: 0.0037771679926663637\n",
      "91) Loss: 0.0037862351164221764\n",
      "92) Loss: 0.003809998743236065\n",
      "93) Loss: 0.003786452580243349\n",
      "94) Loss: 0.0038009071722626686\n",
      "95) Loss: 0.0037795365788042545\n",
      "96) Loss: 0.003792779054492712\n",
      "97) Loss: 0.003790125483646989\n",
      "98) Loss: 0.003785430919378996\n",
      "99) Loss: 0.0037886067293584347\n"
     ]
    }
   ],
   "source": [
    "criterion2 = nn.MSELoss()\n",
    "train_model(100, model, criterion2, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    preds_good = {x[0]: model(x[0]).numpy() for x, y in test_loader if y[0] == 2}\n",
    "    preds_bad = {x[0]: model(x[0]).numpy() for x, y in test_loader if y[0] != 2}\n",
    "    \n",
    "    errors_good = np.array([criterion2(model(x[0]), x[0]) for x, y in test_loader if y[0] == 2])\n",
    "    errors_bad = np.array([criterion2(model(x[0]), x[0]) for x, y in test_loader if y[0] != 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(8)\n",
    "\n",
    "ax.bar(np.arange(len(errors_good)), errors_good, color='cyan')\n",
    "ax.bar(len(errors_good)+np.arange(len(errors_bad)), errors_bad, color='red')\n",
    "ax.legend(['normal', 'outliers'])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e96b7987e83db2bb51693718935e9a4c90615288c60e4ac5255cd193dc13706d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
