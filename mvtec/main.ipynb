{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE: tuple = (128, 128)\n",
    "ROOT_TEST: str = './data/test'\n",
    "ROOT_TRAIN: str = './data/train'\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_folder = datasets.ImageFolder(root=ROOT_TRAIN, transform=transforms)\n",
    "test_folder = datasets.ImageFolder(root=ROOT_TEST, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_folder, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_folder, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_img(img: np.ndarray):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(8)\n",
    "    if img.shape[0] <= 3:\n",
    "        ax.imshow(np.swapaxes(img, 0, 2))\n",
    "    else:\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderAnomaly(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoencoderAnomaly, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 4, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2))\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4, 16, 2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, 2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = self.encoder(X)\n",
    "        return self.decoder(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, model, criterion, optimizer):\n",
    "    for epoch in range(epochs):\n",
    "        loss_batch = 0\n",
    "        for sample, _ in train_loader:\n",
    "            pred = model(sample)\n",
    "            loss = criterion(pred, sample)\n",
    "            loss_batch += loss.detach()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'{epoch}) Loss: {loss_batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoencoderAnomaly()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Loss: 2.7768945693969727\n",
      "1) Loss: 2.773282051086426\n",
      "2) Loss: 2.769437313079834\n",
      "3) Loss: 2.7651326656341553\n",
      "4) Loss: 2.760310173034668\n",
      "5) Loss: 2.7546396255493164\n",
      "6) Loss: 2.7472212314605713\n",
      "7) Loss: 2.737391471862793\n",
      "8) Loss: 2.724660634994507\n",
      "9) Loss: 2.7098681926727295\n",
      "10) Loss: 2.6955394744873047\n",
      "11) Loss: 2.6842665672302246\n",
      "12) Loss: 2.6771397590637207\n",
      "13) Loss: 2.671403408050537\n",
      "14) Loss: 2.6643993854522705\n",
      "15) Loss: 2.658352851867676\n",
      "16) Loss: 2.6534981727600098\n",
      "17) Loss: 2.6497273445129395\n",
      "18) Loss: 2.6459977626800537\n",
      "19) Loss: 2.643131732940674\n",
      "20) Loss: 2.641004800796509\n",
      "21) Loss: 2.639070510864258\n",
      "22) Loss: 2.6373894214630127\n",
      "23) Loss: 2.636362075805664\n",
      "24) Loss: 2.6353845596313477\n",
      "25) Loss: 2.634629487991333\n",
      "26) Loss: 2.633758306503296\n",
      "27) Loss: 2.6333062648773193\n",
      "28) Loss: 2.6326823234558105\n",
      "29) Loss: 2.632275342941284\n",
      "30) Loss: 2.632141351699829\n",
      "31) Loss: 2.6317856311798096\n",
      "32) Loss: 2.631682872772217\n",
      "33) Loss: 2.6314425468444824\n",
      "34) Loss: 2.6312856674194336\n",
      "35) Loss: 2.631187677383423\n",
      "36) Loss: 2.631175994873047\n",
      "37) Loss: 2.630990505218506\n",
      "38) Loss: 2.6307759284973145\n",
      "39) Loss: 2.6311862468719482\n",
      "40) Loss: 2.630913496017456\n",
      "41) Loss: 2.6308529376983643\n",
      "42) Loss: 2.630833148956299\n",
      "43) Loss: 2.6309702396392822\n",
      "44) Loss: 2.630929946899414\n",
      "45) Loss: 2.6306509971618652\n",
      "46) Loss: 2.6306772232055664\n",
      "47) Loss: 2.630760669708252\n",
      "48) Loss: 2.6307690143585205\n",
      "49) Loss: 2.6308181285858154\n"
     ]
    }
   ],
   "source": [
    "train_model(50, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Loss: 2.630657434463501\n",
      "1) Loss: 2.630586862564087\n",
      "2) Loss: 2.630702495574951\n",
      "3) Loss: 2.6305885314941406\n",
      "4) Loss: 2.6306395530700684\n",
      "5) Loss: 2.630746841430664\n",
      "6) Loss: 2.6306209564208984\n",
      "7) Loss: 2.630605697631836\n",
      "8) Loss: 2.630537986755371\n",
      "9) Loss: 2.6306400299072266\n",
      "10) Loss: 2.630643844604492\n",
      "11) Loss: 2.6305086612701416\n",
      "12) Loss: 2.6304516792297363\n",
      "13) Loss: 2.6304821968078613\n",
      "14) Loss: 2.6305289268493652\n",
      "15) Loss: 2.630631446838379\n",
      "16) Loss: 2.6304914951324463\n",
      "17) Loss: 2.6307032108306885\n",
      "18) Loss: 2.6304140090942383\n",
      "19) Loss: 2.630546808242798\n",
      "20) Loss: 2.6304256916046143\n",
      "21) Loss: 2.630448818206787\n",
      "22) Loss: 2.6304619312286377\n",
      "23) Loss: 2.6304049491882324\n",
      "24) Loss: 2.630359649658203\n",
      "25) Loss: 2.630424976348877\n",
      "26) Loss: 2.630340576171875\n",
      "27) Loss: 2.63027286529541\n",
      "28) Loss: 2.6303701400756836\n",
      "29) Loss: 2.6302361488342285\n",
      "30) Loss: 2.6301581859588623\n",
      "31) Loss: 2.6302249431610107\n",
      "32) Loss: 2.630133867263794\n",
      "33) Loss: 2.630181312561035\n",
      "34) Loss: 2.6300034523010254\n",
      "35) Loss: 2.6300296783447266\n",
      "36) Loss: 2.6303014755249023\n",
      "37) Loss: 2.6300017833709717\n",
      "38) Loss: 2.6301074028015137\n",
      "39) Loss: 2.6300745010375977\n",
      "40) Loss: 2.6300764083862305\n",
      "41) Loss: 2.630190372467041\n",
      "42) Loss: 2.6299822330474854\n",
      "43) Loss: 2.630030632019043\n",
      "44) Loss: 2.629998207092285\n",
      "45) Loss: 2.6298952102661133\n",
      "46) Loss: 2.630000591278076\n",
      "47) Loss: 2.6300556659698486\n",
      "48) Loss: 2.62992525100708\n",
      "49) Loss: 2.629849910736084\n",
      "50) Loss: 2.629987955093384\n",
      "51) Loss: 2.629903793334961\n",
      "52) Loss: 2.62992787361145\n",
      "53) Loss: 2.629629135131836\n",
      "54) Loss: 2.6297659873962402\n",
      "55) Loss: 2.629716396331787\n",
      "56) Loss: 2.6297359466552734\n",
      "57) Loss: 2.629746437072754\n",
      "58) Loss: 2.629775047302246\n",
      "59) Loss: 2.6296916007995605\n",
      "60) Loss: 2.6296377182006836\n",
      "61) Loss: 2.629884719848633\n",
      "62) Loss: 2.629535675048828\n",
      "63) Loss: 2.6294846534729004\n",
      "64) Loss: 2.629526138305664\n",
      "65) Loss: 2.6296474933624268\n",
      "66) Loss: 2.62939715385437\n",
      "67) Loss: 2.629523992538452\n",
      "68) Loss: 2.62939190864563\n",
      "69) Loss: 2.629255771636963\n",
      "70) Loss: 2.629315137863159\n",
      "71) Loss: 2.629284381866455\n",
      "72) Loss: 2.6293609142303467\n",
      "73) Loss: 2.6292779445648193\n",
      "74) Loss: 2.6292295455932617\n",
      "75) Loss: 2.62918758392334\n",
      "76) Loss: 2.6290791034698486\n",
      "77) Loss: 2.6290369033813477\n",
      "78) Loss: 2.628988265991211\n",
      "79) Loss: 2.628880739212036\n",
      "80) Loss: 2.6288464069366455\n",
      "81) Loss: 2.6287574768066406\n",
      "82) Loss: 2.6288418769836426\n",
      "83) Loss: 2.628617763519287\n",
      "84) Loss: 2.628709316253662\n",
      "85) Loss: 2.628605842590332\n",
      "86) Loss: 2.6283771991729736\n",
      "87) Loss: 2.628347396850586\n",
      "88) Loss: 2.6283388137817383\n",
      "89) Loss: 2.6281442642211914\n",
      "90) Loss: 2.628263473510742\n",
      "91) Loss: 2.6280312538146973\n",
      "92) Loss: 2.628098487854004\n",
      "93) Loss: 2.6280415058135986\n",
      "94) Loss: 2.6279244422912598\n",
      "95) Loss: 2.6279335021972656\n",
      "96) Loss: 2.6275529861450195\n",
      "97) Loss: 2.62756085395813\n",
      "98) Loss: 2.627265691757202\n",
      "99) Loss: 2.627455949783325\n"
     ]
    }
   ],
   "source": [
    "train_model(100, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Loss: 2.627014636993408\n",
      "1) Loss: 2.6270885467529297\n",
      "2) Loss: 2.6268937587738037\n",
      "3) Loss: 2.6268773078918457\n",
      "4) Loss: 2.626683235168457\n",
      "5) Loss: 2.6264703273773193\n",
      "6) Loss: 2.6263158321380615\n",
      "7) Loss: 2.6261937618255615\n",
      "8) Loss: 2.6258773803710938\n",
      "9) Loss: 2.6259920597076416\n",
      "10) Loss: 2.6256332397460938\n",
      "11) Loss: 2.625359535217285\n",
      "12) Loss: 2.6250696182250977\n",
      "13) Loss: 2.6246516704559326\n",
      "14) Loss: 2.6246161460876465\n",
      "15) Loss: 2.6243131160736084\n",
      "16) Loss: 2.6240758895874023\n",
      "17) Loss: 2.623504638671875\n",
      "18) Loss: 2.62307071685791\n",
      "19) Loss: 2.622715950012207\n",
      "20) Loss: 2.622292995452881\n",
      "21) Loss: 2.621829032897949\n",
      "22) Loss: 2.621274471282959\n",
      "23) Loss: 2.620908498764038\n",
      "24) Loss: 2.620305061340332\n",
      "25) Loss: 2.619925022125244\n",
      "26) Loss: 2.6191539764404297\n",
      "27) Loss: 2.6187610626220703\n",
      "28) Loss: 2.6183371543884277\n",
      "29) Loss: 2.6178038120269775\n",
      "30) Loss: 2.617363452911377\n",
      "31) Loss: 2.6169939041137695\n",
      "32) Loss: 2.6166205406188965\n",
      "33) Loss: 2.6163277626037598\n",
      "34) Loss: 2.615891933441162\n",
      "35) Loss: 2.615557909011841\n",
      "36) Loss: 2.615192174911499\n",
      "37) Loss: 2.614967107772827\n",
      "38) Loss: 2.6146819591522217\n",
      "39) Loss: 2.6143887042999268\n",
      "40) Loss: 2.614161491394043\n",
      "41) Loss: 2.6141045093536377\n",
      "42) Loss: 2.613931179046631\n",
      "43) Loss: 2.614132881164551\n",
      "44) Loss: 2.613961935043335\n",
      "45) Loss: 2.6136178970336914\n",
      "46) Loss: 2.613365411758423\n",
      "47) Loss: 2.613311767578125\n",
      "48) Loss: 2.6130928993225098\n",
      "49) Loss: 2.613101005554199\n",
      "50) Loss: 2.6130528450012207\n",
      "51) Loss: 2.612757682800293\n",
      "52) Loss: 2.6129045486450195\n",
      "53) Loss: 2.6128783226013184\n",
      "54) Loss: 2.612623453140259\n",
      "55) Loss: 2.612832546234131\n",
      "56) Loss: 2.6125600337982178\n",
      "57) Loss: 2.6125364303588867\n",
      "58) Loss: 2.6125447750091553\n",
      "59) Loss: 2.6123976707458496\n",
      "60) Loss: 2.6123292446136475\n",
      "61) Loss: 2.612434148788452\n",
      "62) Loss: 2.6122264862060547\n",
      "63) Loss: 2.612180709838867\n",
      "64) Loss: 2.6121182441711426\n",
      "65) Loss: 2.6120057106018066\n",
      "66) Loss: 2.612183094024658\n",
      "67) Loss: 2.6120808124542236\n",
      "68) Loss: 2.612032413482666\n",
      "69) Loss: 2.612149238586426\n",
      "70) Loss: 2.612016201019287\n",
      "71) Loss: 2.6119916439056396\n",
      "72) Loss: 2.61199951171875\n",
      "73) Loss: 2.6118431091308594\n",
      "74) Loss: 2.6119391918182373\n",
      "75) Loss: 2.611987590789795\n",
      "76) Loss: 2.6118593215942383\n",
      "77) Loss: 2.6117262840270996\n",
      "78) Loss: 2.611814498901367\n",
      "79) Loss: 2.6118757724761963\n",
      "80) Loss: 2.611876964569092\n",
      "81) Loss: 2.6119000911712646\n",
      "82) Loss: 2.612006664276123\n",
      "83) Loss: 2.611870050430298\n",
      "84) Loss: 2.611903429031372\n",
      "85) Loss: 2.6118316650390625\n",
      "86) Loss: 2.6118524074554443\n",
      "87) Loss: 2.6115939617156982\n",
      "88) Loss: 2.6116747856140137\n",
      "89) Loss: 2.6116600036621094\n",
      "90) Loss: 2.611752986907959\n",
      "91) Loss: 2.61163592338562\n",
      "92) Loss: 2.6114959716796875\n",
      "93) Loss: 2.611607074737549\n",
      "94) Loss: 2.6115317344665527\n",
      "95) Loss: 2.611682415008545\n",
      "96) Loss: 2.6119155883789062\n",
      "97) Loss: 2.6116814613342285\n",
      "98) Loss: 2.6114370822906494\n",
      "99) Loss: 2.6116514205932617\n",
      "100) Loss: 2.6115925312042236\n",
      "101) Loss: 2.6116783618927\n",
      "102) Loss: 2.6114327907562256\n",
      "103) Loss: 2.611635446548462\n",
      "104) Loss: 2.6116321086883545\n",
      "105) Loss: 2.611499309539795\n",
      "106) Loss: 2.61149001121521\n",
      "107) Loss: 2.611419677734375\n",
      "108) Loss: 2.611684799194336\n",
      "109) Loss: 2.6116695404052734\n",
      "110) Loss: 2.611581802368164\n",
      "111) Loss: 2.611532688140869\n",
      "112) Loss: 2.6116271018981934\n",
      "113) Loss: 2.6115944385528564\n",
      "114) Loss: 2.611609935760498\n",
      "115) Loss: 2.6115481853485107\n",
      "116) Loss: 2.6114931106567383\n",
      "117) Loss: 2.6113603115081787\n",
      "118) Loss: 2.6113998889923096\n",
      "119) Loss: 2.611393928527832\n",
      "120) Loss: 2.611527919769287\n",
      "121) Loss: 2.611544609069824\n",
      "122) Loss: 2.611243724822998\n",
      "123) Loss: 2.6115658283233643\n",
      "124) Loss: 2.611501455307007\n",
      "125) Loss: 2.6113855838775635\n",
      "126) Loss: 2.6114039421081543\n",
      "127) Loss: 2.611488103866577\n",
      "128) Loss: 2.6112773418426514\n",
      "129) Loss: 2.611355781555176\n",
      "130) Loss: 2.6115121841430664\n",
      "131) Loss: 2.6112799644470215\n",
      "132) Loss: 2.6115036010742188\n",
      "133) Loss: 2.611602544784546\n",
      "134) Loss: 2.6114230155944824\n",
      "135) Loss: 2.611440658569336\n",
      "136) Loss: 2.6114635467529297\n",
      "137) Loss: 2.6114985942840576\n",
      "138) Loss: 2.6115665435791016\n",
      "139) Loss: 2.611542224884033\n",
      "140) Loss: 2.611431121826172\n",
      "141) Loss: 2.611328363418579\n",
      "142) Loss: 2.6114845275878906\n",
      "143) Loss: 2.61132550239563\n",
      "144) Loss: 2.611255168914795\n",
      "145) Loss: 2.611506462097168\n",
      "146) Loss: 2.611562967300415\n",
      "147) Loss: 2.611471652984619\n",
      "148) Loss: 2.6114070415496826\n",
      "149) Loss: 2.6112687587738037\n",
      "150) Loss: 2.611415147781372\n",
      "151) Loss: 2.6112782955169678\n",
      "152) Loss: 2.6114044189453125\n",
      "153) Loss: 2.611400604248047\n",
      "154) Loss: 2.6114633083343506\n",
      "155) Loss: 2.611476182937622\n",
      "156) Loss: 2.611257314682007\n",
      "157) Loss: 2.611469268798828\n",
      "158) Loss: 2.611320734024048\n",
      "159) Loss: 2.6113924980163574\n",
      "160) Loss: 2.611396312713623\n",
      "161) Loss: 2.611337900161743\n",
      "162) Loss: 2.611309051513672\n",
      "163) Loss: 2.611284017562866\n",
      "164) Loss: 2.611354112625122\n",
      "165) Loss: 2.61129093170166\n",
      "166) Loss: 2.61142635345459\n",
      "167) Loss: 2.6111605167388916\n",
      "168) Loss: 2.6114182472229004\n",
      "169) Loss: 2.6113967895507812\n",
      "170) Loss: 2.6112966537475586\n",
      "171) Loss: 2.6113693714141846\n",
      "172) Loss: 2.611328363418579\n",
      "173) Loss: 2.6113061904907227\n",
      "174) Loss: 2.611321449279785\n",
      "175) Loss: 2.611392021179199\n",
      "176) Loss: 2.611161470413208\n",
      "177) Loss: 2.6114437580108643\n",
      "178) Loss: 2.6113476753234863\n",
      "179) Loss: 2.6113245487213135\n",
      "180) Loss: 2.611398696899414\n",
      "181) Loss: 2.611254930496216\n",
      "182) Loss: 2.6114141941070557\n",
      "183) Loss: 2.611426591873169\n",
      "184) Loss: 2.6114542484283447\n",
      "185) Loss: 2.611166000366211\n",
      "186) Loss: 2.6112306118011475\n",
      "187) Loss: 2.6112802028656006\n",
      "188) Loss: 2.6114392280578613\n",
      "189) Loss: 2.6112632751464844\n",
      "190) Loss: 2.6112849712371826\n",
      "191) Loss: 2.611264705657959\n",
      "192) Loss: 2.6112887859344482\n",
      "193) Loss: 2.611340284347534\n",
      "194) Loss: 2.611304759979248\n",
      "195) Loss: 2.6114373207092285\n",
      "196) Loss: 2.611267566680908\n",
      "197) Loss: 2.6113011837005615\n",
      "198) Loss: 2.6115410327911377\n",
      "199) Loss: 2.611459493637085\n",
      "200) Loss: 2.6111936569213867\n",
      "201) Loss: 2.611147165298462\n",
      "202) Loss: 2.6112804412841797\n",
      "203) Loss: 2.611285924911499\n",
      "204) Loss: 2.611253499984741\n",
      "205) Loss: 2.611154556274414\n",
      "206) Loss: 2.6111817359924316\n",
      "207) Loss: 2.6112260818481445\n",
      "208) Loss: 2.611330986022949\n",
      "209) Loss: 2.6111900806427\n",
      "210) Loss: 2.6112561225891113\n",
      "211) Loss: 2.611254930496216\n",
      "212) Loss: 2.6113433837890625\n",
      "213) Loss: 2.611238479614258\n",
      "214) Loss: 2.611293315887451\n",
      "215) Loss: 2.6112115383148193\n",
      "216) Loss: 2.6112124919891357\n",
      "217) Loss: 2.6112265586853027\n",
      "218) Loss: 2.6114115715026855\n",
      "219) Loss: 2.6112074851989746\n",
      "220) Loss: 2.611180067062378\n",
      "221) Loss: 2.611201286315918\n",
      "222) Loss: 2.6113462448120117\n",
      "223) Loss: 2.611311674118042\n",
      "224) Loss: 2.6111714839935303\n",
      "225) Loss: 2.6110472679138184\n",
      "226) Loss: 2.611133098602295\n",
      "227) Loss: 2.6112077236175537\n",
      "228) Loss: 2.611238718032837\n",
      "229) Loss: 2.611283302307129\n",
      "230) Loss: 2.6111788749694824\n",
      "231) Loss: 2.6112937927246094\n",
      "232) Loss: 2.6110005378723145\n",
      "233) Loss: 2.6114180088043213\n",
      "234) Loss: 2.6113405227661133\n",
      "235) Loss: 2.6114492416381836\n",
      "236) Loss: 2.6112449169158936\n",
      "237) Loss: 2.611198902130127\n",
      "238) Loss: 2.6111536026000977\n",
      "239) Loss: 2.611330986022949\n",
      "240) Loss: 2.611208915710449\n",
      "241) Loss: 2.611164093017578\n",
      "242) Loss: 2.611112594604492\n",
      "243) Loss: 2.6112453937530518\n",
      "244) Loss: 2.611201524734497\n",
      "245) Loss: 2.6112852096557617\n",
      "246) Loss: 2.6113624572753906\n",
      "247) Loss: 2.6110646724700928\n",
      "248) Loss: 2.6111693382263184\n",
      "249) Loss: 2.6111836433410645\n",
      "250) Loss: 2.611250877380371\n",
      "251) Loss: 2.610966444015503\n",
      "252) Loss: 2.611304998397827\n",
      "253) Loss: 2.61136794090271\n",
      "254) Loss: 2.6112046241760254\n",
      "255) Loss: 2.611313819885254\n",
      "256) Loss: 2.6111764907836914\n",
      "257) Loss: 2.6111392974853516\n",
      "258) Loss: 2.610926628112793\n",
      "259) Loss: 2.61130690574646\n",
      "260) Loss: 2.61112117767334\n",
      "261) Loss: 2.611161470413208\n",
      "262) Loss: 2.6111888885498047\n",
      "263) Loss: 2.611254930496216\n",
      "264) Loss: 2.611156702041626\n",
      "265) Loss: 2.6111817359924316\n",
      "266) Loss: 2.6110944747924805\n",
      "267) Loss: 2.611173152923584\n",
      "268) Loss: 2.61110258102417\n",
      "269) Loss: 2.6112027168273926\n",
      "270) Loss: 2.611112117767334\n",
      "271) Loss: 2.611262321472168\n",
      "272) Loss: 2.6111538410186768\n",
      "273) Loss: 2.6110262870788574\n",
      "274) Loss: 2.611262798309326\n",
      "275) Loss: 2.611090660095215\n",
      "276) Loss: 2.611124038696289\n",
      "277) Loss: 2.6111912727355957\n",
      "278) Loss: 2.6111974716186523\n",
      "279) Loss: 2.61104154586792\n",
      "280) Loss: 2.611118793487549\n",
      "281) Loss: 2.6111512184143066\n",
      "282) Loss: 2.611311435699463\n",
      "283) Loss: 2.611262321472168\n",
      "284) Loss: 2.6112234592437744\n",
      "285) Loss: 2.6110000610351562\n",
      "286) Loss: 2.6112585067749023\n",
      "287) Loss: 2.611293315887451\n",
      "288) Loss: 2.611178159713745\n",
      "289) Loss: 2.6111247539520264\n",
      "290) Loss: 2.61114764213562\n",
      "291) Loss: 2.6111459732055664\n",
      "292) Loss: 2.611184597015381\n",
      "293) Loss: 2.6112515926361084\n",
      "294) Loss: 2.611130952835083\n",
      "295) Loss: 2.611036777496338\n",
      "296) Loss: 2.611067533493042\n",
      "297) Loss: 2.6111602783203125\n",
      "298) Loss: 2.611180305480957\n",
      "299) Loss: 2.6111741065979004\n",
      "300) Loss: 2.6111083030700684\n",
      "301) Loss: 2.611238956451416\n",
      "302) Loss: 2.6111979484558105\n",
      "303) Loss: 2.6110928058624268\n",
      "304) Loss: 2.6111044883728027\n",
      "305) Loss: 2.611114263534546\n",
      "306) Loss: 2.6110634803771973\n",
      "307) Loss: 2.611124277114868\n",
      "308) Loss: 2.611107110977173\n",
      "309) Loss: 2.6110153198242188\n",
      "310) Loss: 2.6112070083618164\n",
      "311) Loss: 2.6110472679138184\n",
      "312) Loss: 2.6111786365509033\n",
      "313) Loss: 2.6109962463378906\n",
      "314) Loss: 2.611119508743286\n",
      "315) Loss: 2.611083507537842\n",
      "316) Loss: 2.6110219955444336\n",
      "317) Loss: 2.6109261512756348\n",
      "318) Loss: 2.6110708713531494\n",
      "319) Loss: 2.611065626144409\n",
      "320) Loss: 2.611084461212158\n",
      "321) Loss: 2.6109797954559326\n",
      "322) Loss: 2.611213207244873\n",
      "323) Loss: 2.6111903190612793\n",
      "324) Loss: 2.611212730407715\n",
      "325) Loss: 2.6111598014831543\n",
      "326) Loss: 2.611309051513672\n",
      "327) Loss: 2.6111509799957275\n",
      "328) Loss: 2.611250400543213\n",
      "329) Loss: 2.611104965209961\n",
      "330) Loss: 2.6111738681793213\n",
      "331) Loss: 2.610931396484375\n",
      "332) Loss: 2.6110129356384277\n",
      "333) Loss: 2.6111621856689453\n",
      "334) Loss: 2.6110029220581055\n",
      "335) Loss: 2.611253261566162\n",
      "336) Loss: 2.611104965209961\n",
      "337) Loss: 2.6110501289367676\n",
      "338) Loss: 2.611039161682129\n",
      "339) Loss: 2.61103892326355\n",
      "340) Loss: 2.610934019088745\n",
      "341) Loss: 2.6111104488372803\n",
      "342) Loss: 2.6111130714416504\n",
      "343) Loss: 2.6110291481018066\n",
      "344) Loss: 2.6109955310821533\n",
      "345) Loss: 2.611020803451538\n",
      "346) Loss: 2.611177921295166\n",
      "347) Loss: 2.6111044883728027\n",
      "348) Loss: 2.610973834991455\n",
      "349) Loss: 2.6111788749694824\n",
      "350) Loss: 2.61104154586792\n",
      "351) Loss: 2.6110284328460693\n",
      "352) Loss: 2.611081123352051\n",
      "353) Loss: 2.6109418869018555\n",
      "354) Loss: 2.610988140106201\n",
      "355) Loss: 2.611173152923584\n",
      "356) Loss: 2.6112377643585205\n",
      "357) Loss: 2.6110448837280273\n",
      "358) Loss: 2.61095929145813\n",
      "359) Loss: 2.610999584197998\n",
      "360) Loss: 2.6110754013061523\n",
      "361) Loss: 2.6109678745269775\n",
      "362) Loss: 2.610945701599121\n",
      "363) Loss: 2.6109156608581543\n",
      "364) Loss: 2.6110644340515137\n",
      "365) Loss: 2.611111640930176\n",
      "366) Loss: 2.6110422611236572\n",
      "367) Loss: 2.6110053062438965\n",
      "368) Loss: 2.6110000610351562\n",
      "369) Loss: 2.6109156608581543\n",
      "370) Loss: 2.61116361618042\n",
      "371) Loss: 2.6110219955444336\n",
      "372) Loss: 2.611062526702881\n",
      "373) Loss: 2.6109752655029297\n",
      "374) Loss: 2.6111111640930176\n",
      "375) Loss: 2.6110475063323975\n",
      "376) Loss: 2.6110477447509766\n",
      "377) Loss: 2.6110472679138184\n",
      "378) Loss: 2.611053943634033\n",
      "379) Loss: 2.611138105392456\n",
      "380) Loss: 2.6110846996307373\n",
      "381) Loss: 2.611117124557495\n",
      "382) Loss: 2.6111087799072266\n",
      "383) Loss: 2.6109936237335205\n",
      "384) Loss: 2.6110310554504395\n",
      "385) Loss: 2.6109490394592285\n",
      "386) Loss: 2.610973834991455\n",
      "387) Loss: 2.6109695434570312\n",
      "388) Loss: 2.6110024452209473\n",
      "389) Loss: 2.610790252685547\n",
      "390) Loss: 2.611011028289795\n",
      "391) Loss: 2.6109116077423096\n",
      "392) Loss: 2.6110482215881348\n",
      "393) Loss: 2.610999822616577\n",
      "394) Loss: 2.611090660095215\n",
      "395) Loss: 2.6109964847564697\n",
      "396) Loss: 2.6110763549804688\n",
      "397) Loss: 2.6111297607421875\n",
      "398) Loss: 2.610900640487671\n",
      "399) Loss: 2.611112356185913\n",
      "400) Loss: 2.611016273498535\n",
      "401) Loss: 2.610983371734619\n",
      "402) Loss: 2.611001968383789\n",
      "403) Loss: 2.6109747886657715\n",
      "404) Loss: 2.6110925674438477\n",
      "405) Loss: 2.610877752304077\n",
      "406) Loss: 2.6110894680023193\n",
      "407) Loss: 2.611010789871216\n",
      "408) Loss: 2.6109776496887207\n",
      "409) Loss: 2.6109604835510254\n",
      "410) Loss: 2.611135959625244\n",
      "411) Loss: 2.611022710800171\n",
      "412) Loss: 2.6110875606536865\n",
      "413) Loss: 2.6110358238220215\n",
      "414) Loss: 2.6110267639160156\n",
      "415) Loss: 2.6108808517456055\n",
      "416) Loss: 2.610907554626465\n",
      "417) Loss: 2.610776662826538\n",
      "418) Loss: 2.61106276512146\n",
      "419) Loss: 2.6111087799072266\n",
      "420) Loss: 2.611022472381592\n",
      "421) Loss: 2.611097812652588\n",
      "422) Loss: 2.6111254692077637\n",
      "423) Loss: 2.6109580993652344\n",
      "424) Loss: 2.611110210418701\n",
      "425) Loss: 2.6110215187072754\n",
      "426) Loss: 2.611100912094116\n",
      "427) Loss: 2.611011028289795\n",
      "428) Loss: 2.610914468765259\n",
      "429) Loss: 2.610900640487671\n",
      "430) Loss: 2.6109800338745117\n",
      "431) Loss: 2.611008644104004\n",
      "432) Loss: 2.6109352111816406\n",
      "433) Loss: 2.6109673976898193\n",
      "434) Loss: 2.6108274459838867\n",
      "435) Loss: 2.6107852458953857\n",
      "436) Loss: 2.6111092567443848\n",
      "437) Loss: 2.6110646724700928\n",
      "438) Loss: 2.6110126972198486\n",
      "439) Loss: 2.6111156940460205\n",
      "440) Loss: 2.610999584197998\n",
      "441) Loss: 2.6110377311706543\n",
      "442) Loss: 2.610926628112793\n",
      "443) Loss: 2.6108758449554443\n",
      "444) Loss: 2.6109366416931152\n",
      "445) Loss: 2.6110377311706543\n",
      "446) Loss: 2.610905885696411\n",
      "447) Loss: 2.6110668182373047\n",
      "448) Loss: 2.61094331741333\n",
      "449) Loss: 2.610934257507324\n",
      "450) Loss: 2.6108317375183105\n",
      "451) Loss: 2.6110379695892334\n",
      "452) Loss: 2.6108646392822266\n",
      "453) Loss: 2.611021041870117\n",
      "454) Loss: 2.6110150814056396\n",
      "455) Loss: 2.6107468605041504\n",
      "456) Loss: 2.610841989517212\n",
      "457) Loss: 2.6109254360198975\n",
      "458) Loss: 2.6109426021575928\n",
      "459) Loss: 2.6107466220855713\n",
      "460) Loss: 2.610919713973999\n",
      "461) Loss: 2.610898494720459\n",
      "462) Loss: 2.6108591556549072\n",
      "463) Loss: 2.6110148429870605\n",
      "464) Loss: 2.6107869148254395\n",
      "465) Loss: 2.6108663082122803\n",
      "466) Loss: 2.61091685295105\n",
      "467) Loss: 2.6110148429870605\n",
      "468) Loss: 2.6109073162078857\n",
      "469) Loss: 2.611074924468994\n",
      "470) Loss: 2.6109869480133057\n",
      "471) Loss: 2.611016273498535\n",
      "472) Loss: 2.610853672027588\n",
      "473) Loss: 2.610896587371826\n",
      "474) Loss: 2.6110904216766357\n",
      "475) Loss: 2.6109371185302734\n",
      "476) Loss: 2.6109354496002197\n",
      "477) Loss: 2.6108415126800537\n",
      "478) Loss: 2.6110587120056152\n",
      "479) Loss: 2.610872268676758\n",
      "480) Loss: 2.610839366912842\n",
      "481) Loss: 2.6108996868133545\n",
      "482) Loss: 2.6108949184417725\n",
      "483) Loss: 2.6109399795532227\n",
      "484) Loss: 2.6109938621520996\n",
      "485) Loss: 2.610921621322632\n",
      "486) Loss: 2.6109700202941895\n",
      "487) Loss: 2.6110005378723145\n",
      "488) Loss: 2.6110522747039795\n",
      "489) Loss: 2.611023187637329\n",
      "490) Loss: 2.6109585762023926\n",
      "491) Loss: 2.61098575592041\n",
      "492) Loss: 2.6110033988952637\n",
      "493) Loss: 2.6109609603881836\n",
      "494) Loss: 2.6108264923095703\n",
      "495) Loss: 2.610734224319458\n",
      "496) Loss: 2.6109066009521484\n",
      "497) Loss: 2.6110424995422363\n",
      "498) Loss: 2.610879421234131\n",
      "499) Loss: 2.6108808517456055\n"
     ]
    }
   ],
   "source": [
    "train_model(500, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Loss: 0.003808812005445361\n",
      "1) Loss: 0.0038185077719390392\n",
      "2) Loss: 0.0038108602166175842\n",
      "3) Loss: 0.0038193892687559128\n",
      "4) Loss: 0.003818931058049202\n"
     ]
    }
   ],
   "source": [
    "criterion2 = nn.MSELoss()\n",
    "train_model(5, model, criterion2, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    preds_good = {x[0]: model(x[0]).numpy() for x, y in test_loader if y[0] == 2}\n",
    "    preds_bad = {x[0]: model(x[0]).numpy() for x, y in test_loader if y[0] != 2}\n",
    "    \n",
    "    errors_good = np.array([criterion(model(x[0]), x[0]) for x, y in test_loader if y[0] == 2])\n",
    "    errors_bad = np.array([criterion(model(x[0]), x[0]) for x, y in test_loader if y[0] != 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00094062 0.00095159 0.00081913 0.00082977 0.00087352 0.00083083\n",
      " 0.00084485 0.0004583  0.00058551 0.00061908 0.00064013 0.00069257\n",
      " 0.00070475 0.00068778 0.00083244 0.0008019  0.00088228 0.000869\n",
      " 0.00082257]\n",
      "[0.00104259 0.00177463 0.00133399 0.00087512 0.00157849 0.00138802\n",
      " 0.00089707 0.00087289 0.00124015 0.00127046 0.0006467  0.00080136\n",
      " 0.00255629 0.00116651 0.00206731 0.00160198 0.00093555 0.00135028\n",
      " 0.00109215 0.0011885  0.00118208 0.00154101 0.00121386 0.00144873\n",
      " 0.00142143 0.00244783 0.00154157 0.00150902 0.00094461 0.00076515\n",
      " 0.00077449 0.00062671 0.00119523 0.00128816 0.00102903 0.00116381\n",
      " 0.00116384 0.00081112 0.00072153 0.00157325 0.00115694 0.001335\n",
      " 0.00156383 0.00178909 0.00108289 0.00118566 0.00141075 0.00062076\n",
      " 0.00110626 0.00139179 0.00178194 0.00214578 0.00190578 0.00153486\n",
      " 0.00076598 0.0008599  0.00102246 0.00078257 0.00248656 0.00124614]\n"
     ]
    }
   ],
   "source": [
    "print(errors_good)\n",
    "print(errors_bad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e96b7987e83db2bb51693718935e9a4c90615288c60e4ac5255cd193dc13706d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
